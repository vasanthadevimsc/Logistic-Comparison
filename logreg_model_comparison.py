# -*- coding: utf-8 -*-
"""logreg_model_comparison.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ar_oqDNlpbCi3jXwEbCVNBPVQyB5TACl
"""

import numpy as np
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split

np.random.seed(42)
X, y = make_classification(
    n_samples=500,
    n_features=8,
    n_informative=5,
    n_redundant=0,
    n_clusters_per_class=2,
    class_sep=1.0,
    random_state=42
)


X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.25, random_state=42
)
def sigmoid(z):
    return 1 / (1 + np.exp(-z))
# Log Loss
def log_loss(y_true, y_pred):
    eps = 1e-15  # numerical stability
    y_pred = np.clip(y_pred, eps, 1 - eps)
    return -np.mean(
        y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred)
    )
# Logistic Regression Class (Batch Gradient Descent)
class LogisticRegressionScratch:
    def __init__(self, lr=0.01, n_iters=1000):
        self.lr = lr
        self.n_iters = n_iters
        self.weights = None
        self.bias = None
        self.losses = []

    def fit(self, X, y):
        n_samples, n_features = X.shape
        self.weights = np.zeros(n_features)
        self.bias = 0.0

        for _ in range(self.n_iters):
            linear_model = np.dot(X, self.weights) + self.bias
            y_pred = sigmoid(linear_model)

            # Gradients
            dw = (1 / n_samples) * np.dot(X.T, (y_pred - y))
            db = (1 / n_samples) * np.sum(y_pred - y)

            # Update
            self.weights -= self.lr * dw
            self.bias -= self.lr * db

            # Track loss
            loss = log_loss(y, y_pred)
            self.losses.append(loss)

    def predict_proba(self, X):
        linear_model = np.dot(X, self.weights) + self.bias
        return sigmoid(linear_model)

    def predict(self, X, threshold=0.5):
        return (self.predict_proba(X) >= threshold).astype(int)

mean = X_train.mean(axis=0)
std = X_train.std(axis=0)

X_train_scaled = (X_train - mean) / std
X_test_scaled = (X_test - mean) / std

model_scratch = LogisticRegressionScratch(lr=0.1,n_iters=2000)
model_scratch.fit(X_train_scaled, y_train)

from sklearn.linear_model import LogisticRegression

sk_model = LogisticRegression(penalty=None,solver="lbfgs", max_iter=2000)
sk_model.fit(X_train_scaled, y_train)

print("Scratch model weights:",model_scratch.weights)
print("Scratch model bias:",model_scratch.bias)
print("\nScikit-learn weights:",sk_model.coef_[0])
print("Scikit-learn bias:",sk_model.intercept_[0])

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

def evaluate_model(y_true, y_pred, name="Model"):
    print(f"\n{name} Performance:")
    print("Accuracy :", accuracy_score(y_true, y_pred))
    print("Precision:", precision_score(y_true, y_pred))
    print("Recall   :", recall_score(y_true, y_pred))
    print("F1 Score :", f1_score(y_true, y_pred))

y_pred_scratch = model_scratch.predict(X_test_scaled)
y_pred_sklearn = sk_model.predict(X_test_scaled)
evaluate_model(y_test, y_pred_scratch, "Scratch Logistic Regression")
evaluate_model(y_test, y_pred_sklearn, "Scikit-learn Logistic Regression")

import matplotlib.pyplot as plt
plt.figure(figsize=(7, 4))
plt.plot(model_scratch.losses, color="purple")
plt.xlabel("Iterations")
plt.ylabel("Log Loss")
plt.title("Training Loss Curve (Scratch Logistic Regression)")
plt.grid(True)
plt.show()

proba_scratch = model_scratch.predict_proba(X_test_scaled)
proba_sklearn = sk_model.predict_proba(X_test_scaled)[:, 1]

plt.figure(figsize=(6, 6))
plt.scatter(proba_scratch, proba_sklearn, alpha=0.6)
plt.plot([0, 1], [0, 1], 'r--')
plt.xlabel("Scratch Model Probabilities")
plt.ylabel("Scikit-learn Probabilities")
plt.title("Probability Predictions Comparison")
plt.grid(True)
plt.show()
plt.figure(figsize=(7, 5))

plt.scatter(X_test_scaled[:, 0], X_test_scaled[:, 1],c=y_test,cmap="coolwarm",alpha=0.6,edgecolors="k")

plt.xlabel("Feature 1 (scaled)")
plt.ylabel("Feature 2 (scaled)")
plt.title("Test Data Projection (Colored by True Label)")
plt.show()